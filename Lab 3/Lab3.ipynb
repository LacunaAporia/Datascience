{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rbbDomQazzsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27637778-8a9f-48e7-e4ac-c800451ddd54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacytextblob\n",
            "  Downloading spacytextblob-4.0.0-py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: textblob<0.16.0,>=0.15.3 in /usr/local/lib/python3.7/dist-packages (from spacytextblob) (0.15.3)\n",
            "Requirement already satisfied: spacy<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from spacytextblob) (3.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (0.6.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (8.1.4)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0,>=3.0->spacytextblob) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0,>=3.0->spacytextblob) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0,>=3.0->spacytextblob) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (1.24.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob<0.16.0,>=0.15.3->spacytextblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (2022.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0,>=3.0->spacytextblob) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0,>=3.0->spacytextblob) (0.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0,>=3.0->spacytextblob) (2.0.1)\n",
            "Installing collected packages: spacytextblob\n",
            "Successfully installed spacytextblob-4.0.0\n"
          ]
        }
      ],
      "source": [
        "# 1. Using news article titles, provide a list of all the countries that are mentioned in the\n",
        "# month of March 2017. Briefly comment on your findings.\n",
        "\n",
        "!pip3 install spacytextblob\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime # for grabbing date ranges\n",
        "import spacy # for natural language processing\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob # for sentiment analysis\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# spacy.explain('JJ')\n",
        "\n",
        "# spacy.explain('JJ')\n",
        "# print(nlp.get_pipe('ner').labels)\n",
        "# print([f'{ent.text}, {ent.label_}, {spacy.explain(ent.label_)}' for ent in doc.ents])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_dates(df):\n",
        "  \"\"\"\n",
        "  Translates dates from various string formats to python Datetime objects. \n",
        "  Adds columns Date: Datetime, month: int, year: int, day: int.\n",
        "  Also tosses out corrupted rows in the dataframe.\n",
        "\n",
        "  :param df: (DataFrame) With column 'date'\n",
        "\n",
        "  :returns: (DataFrame) Cleaned with additional columns Date, month, day, year\n",
        "  \"\"\"\n",
        "  formats = {1: '%d-%b-%y', 0: '%B %d, %Y', 2: '%b %d, %Y'}\n",
        "  dates = []\n",
        "  bad = []\n",
        "  for i, date in enumerate(df['date']):\n",
        "    try:\n",
        "      if date[0].isdigit():\n",
        "        f = formats[1]\n",
        "      elif len(date.split()[0].strip()) > 3:\n",
        "        f = formats[0]\n",
        "      else:\n",
        "        f = formats[2]\n",
        "      dates.append(datetime.strptime(date.strip(), f))\n",
        "    except:\n",
        "      bad.append([date, format, i])\n",
        "  print(bad)\n",
        "  df = df.drop([b[-1] for b in bad])\n",
        "  df['Date'] = dates\n",
        "  df['month'] = pd.DatetimeIndex(dates).month\n",
        "  df['year'] = pd.DatetimeIndex(dates).year\n",
        "  df['day'] = pd.DatetimeIndex(dates).day\n",
        "  return df\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('spacytextblob')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPVQviPLLao4",
        "outputId": "fb9146d1-b285-491c-ee4c-09e2a2645d74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7f26ec3a9790>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fake.csv')\n",
        "# real = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/True.csv')\n",
        "\n",
        "fake = clean_dates(fake)\n",
        "# real = clean_dates(real)\n",
        "fDate = fake[(fake.year == 2017) & (fake.month == 3)]\n",
        "# rDate = real[(real.year == 2017) & (real.month == 3)]\n",
        "\n",
        "\n",
        "countries = set()\n",
        "doc = fDate.title.apply(nlp)\n",
        "for i in range(len(doc)): \n",
        "# print(doc[i].tag_, doc[i].pos)  # - Find number of indexed items in the doc\n",
        " for t in doc.iloc[i].ents:  # - Filtering the elements \n",
        "#  print(t.label_)\n",
        "  if t.label_ == \"GPE\":\n",
        "    countries.add(str(t))\n",
        "countries\n",
        "\n",
        "\n",
        "\n",
        "    # def count_GPE(t):\n",
        "    #   print(t.text, t.label_)\n",
        "    # #  print(doc.apply(count_GPE).most_common(10))\n",
        "    #  # nouns = [\n",
        "    #  #   token.lemma_ for token in doc if \n",
        "    #  #         (not token.is_stop and\n",
        "    #  #          not token.is_punct and\n",
        "    #  #          token.tag_ == \"NNPS\")]\n",
        "    #  gCnt = Counter(t)\n",
        "    #  return gCnt\n",
        "\n",
        "    #print(t.apply(count_GPE)) # .sum().most_common(50))\n",
        "#    print(t.text, t.pos_, t.Label_)\n",
        "\n",
        "  # # counts = doc.apply(count_GPE).sum().most_common(50)\n",
        "  # print(counts)\n",
        "\n",
        "\n",
        "# for chunk in doc.noun_chunks:       # iterate over the noun chunks in the Doc\n",
        "#    print([f'{chunk.text}, {ent.label_}, {spacy.explain(ent.label_)}' for ent in doc.ents])\n",
        "# print([n for n in doc.noun_chunks])\n",
        "\n",
        "\n",
        "#  if token.tag_ == \"NNP\":\n",
        "#   print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
        "#   print(token.text) #, token.pos_, token.dep_)\n",
        "#   print([token.text for t in doc]) # word based tokenization\n",
        "#   print([token.lemma_ for t in doc]) # lemmatization\n",
        "#   print([token.pos_ for t in doc]) # Part of speech tagging\n",
        "#   print([token.tag_ for t in doc])\n",
        "#   print([token.label_ for t in doc])\n",
        "#   print([n for n in doc.noun_chunks]) # Noun phrase parsing\n",
        " # for e in doc.iloc[i].ents:  # e.text, e.start_char, e.end_char, \n",
        "\n",
        "#    c = Counter(e)\n",
        " #   print(c)\n",
        "\n",
        "# print(fake_docs.apply(count_nouns).sum().most_common(20))\n",
        "\n",
        "    # from spacy.tokens import Doc\n",
        "    # city_getter = lambda doc: any(GPE in e for token in (doc))\n",
        "    # Doc.set_extension(\"has_city\", getter=city_getter)\n",
        "    # assert doc._.has_city"
      ],
      "metadata": {
        "id": "AmAkPRGALGPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5dc96d-0202-4a74-926e-6f15c8780c7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['https://100percentfedup.com/served-roy-moore-vietnamletter-veteran-sets-record-straight-honorable-decent-respectable-patriotic-commander-soldier/', <built-in function format>, 9358], ['https://100percentfedup.com/video-hillary-asked-about-trump-i-just-want-to-eat-some-pie/', <built-in function format>, 15507], ['https://100percentfedup.com/12-yr-old-black-conservative-whose-video-to-obama-went-viral-do-you-really-love-america-receives-death-threats-from-left/', <built-in function format>, 15508], ['https://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpg', <built-in function format>, 15839], ['https://fedup.wpengine.com/wp-content/uploads/2015/04/entitled.jpg', <built-in function format>, 15840], ['https://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpg', <built-in function format>, 17432], ['https://fedup.wpengine.com/wp-content/uploads/2015/04/entitled.jpg', <built-in function format>, 17433], ['MSNBC HOST Rudely Assumes Steel Worker Would Never Let His Son Follow in His Footsteps…He Couldn’t Be More Wrong [Video]', <built-in function format>, 18933], ['https://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpg', <built-in function format>, 21869], ['https://fedup.wpengine.com/wp-content/uploads/2015/04/entitled.jpg', <built-in function format>, 21870]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A Mistake',\n",
              " 'ACCIDENTALLY',\n",
              " 'AFGHANISTAN',\n",
              " 'ALABAMA',\n",
              " 'America',\n",
              " 'BRITAIN',\n",
              " 'Berkeley',\n",
              " 'Breitbart',\n",
              " 'Bronx',\n",
              " 'California',\n",
              " 'Cancun',\n",
              " 'Colorado',\n",
              " 'Cut',\n",
              " 'DC',\n",
              " 'Florida',\n",
              " 'Front',\n",
              " 'GERMANY',\n",
              " 'Germany',\n",
              " 'Hawaii',\n",
              " 'Iowa',\n",
              " 'Iraq',\n",
              " 'Key States',\n",
              " 'London',\n",
              " 'MEXICO',\n",
              " 'Mexico',\n",
              " 'Moscow',\n",
              " 'NEW JERSEY',\n",
              " 'PELOSI',\n",
              " 'Palm Beach County Looks',\n",
              " 'Pennsylvania',\n",
              " 'Philadelphia',\n",
              " 'QB',\n",
              " 'Russia',\n",
              " 'San Jose',\n",
              " 'Shackles',\n",
              " 'St. Patrick’s',\n",
              " 'Supposed',\n",
              " 'Sweden',\n",
              " 'Syria',\n",
              " 'TEXAS',\n",
              " 'Texas',\n",
              " 'U.S',\n",
              " 'U.S.',\n",
              " 'UK',\n",
              " 'US',\n",
              " 'Virginia',\n",
              " 'Washington'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# :\n",
        "# import pandas as pd\n",
        "# from datetime import datetime\n",
        "# import spacy\n",
        "# from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "# from collections import Counter\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "# nlp.add_pipe('spacytextblob')\n",
        "\n",
        "# df = pd.read_csv('fake.csv')\n",
        "# filtered_df = df\n",
        "# filtered_df = filtered_df[filtered_df['date'].str.contains(r'(March .* 2017)')]\n",
        "# # filtered_df = filtered_df[filtered_df['date'].str.contains('2017')]\n",
        "# # filtered_df = filtered_df[filtered_df['date'].str.contains('March')]\n",
        "# #print(filtered_df)\n",
        "fDay = fake[(fake.year == 2016) & (fake.month == 11) & (fake.day == 8)]\n",
        "# people = set()\n",
        "doc = fDay.title.apply(nlp)\n",
        "people = Counter()\n",
        "for i in range(len(doc)): \n",
        "# print(doc[i].tag_, doc[i].pos)  # - Find number of indexed items in the doc\n",
        " for t in doc.iloc[i].ents:  # - Filtering the elements \n",
        "  print(t.text, t.label_)\n",
        "  if t.label_ == \"PERSON\":\n",
        "   print(t.text, t.label_)\n",
        "  def top5(doc):\n",
        "   peps = [\n",
        "      token.lemma_ for token in doc if \n",
        "            (not token.is_stop and\n",
        "             not token.is_punct and \n",
        "             t.label_ == \"PERSON\")]\n",
        "   top = Counter(peps)\n",
        "   return top\n",
        "\n",
        "count = doc.apply(top5).sum().most_common(5)\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e2s9Vnxvi2a",
        "outputId": "b0925e2b-ca44-45a0-e084-2177db1c3ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Election Day DATE\n",
            "Trump Calls Running ORG\n",
            "Active Shooter Reported Outside Los Angeles Polling Location WORK_OF_ART\n",
            "Trump Supporter Pulls Gun ORG\n",
            "Trump Tries To MANIPULATE Nevada Vote ORG\n",
            "Shut Down PERSON\n",
            "Shut Down PERSON\n",
            "Couple Married ORG\n",
            "Bill Maher PERSON\n",
            "Bill Maher PERSON\n",
            "Anderson Cooper PERSON\n",
            "Anderson Cooper PERSON\n",
            "Hillary Rally PERSON\n",
            "Hillary Rally PERSON\n",
            "Jon Stewart Returned PERSON\n",
            "Jon Stewart Returned PERSON\n",
            "George PERSON\n",
            "George PERSON\n",
            "Laura Bush PERSON\n",
            "Laura Bush PERSON\n",
            "NEVADA GPE\n",
            "BILL CLINTON Spaces Out ORG\n",
            "HUNDREDS CARDINAL\n",
            "Protect Elections” From WORK_OF_ART\n",
            "Outside Forces…Hacker ORG\n",
            "Guccifer PERSON\n",
            "Guccifer PERSON\n",
            "HUNDREDS CARDINAL\n",
            "Protect Elections” From WORK_OF_ART\n",
            "Outside Forces…Hacker ORG\n",
            "Guccifer PERSON\n",
            "Guccifer PERSON\n",
            "US GPE\n",
            "Voter Fraud Emerges ORG\n",
            "The Wahabi Vote ORG\n",
            "68 percent PERCENT\n",
            "Saudis NORP\n",
            "Hillary Clinton PERSON\n",
            "Hillary Clinton PERSON\n",
            "Clinton PERSON\n",
            "Clinton PERSON\n",
            "Mickey Mouse PERSON\n",
            "Mickey Mouse PERSON\n",
            "Cruella de Vil PERSON\n",
            "Cruella de Vil PERSON\n",
            "US GPE\n",
            "Voter Fraud Emerges ORG\n",
            "The Wahabi Vote ORG\n",
            "68 percent PERCENT\n",
            "Saudis NORP\n",
            "Hillary Clinton PERSON\n",
            "Hillary Clinton PERSON\n",
            "Clinton PERSON\n",
            "Clinton PERSON\n",
            "Mickey Mouse PERSON\n",
            "Mickey Mouse PERSON\n",
            "Cruella de Vil PERSON\n",
            "Cruella de Vil PERSON\n",
            "[(' ', 13), ('Trump', 10), ('trump', 6), ('vote', 5), ('Hillary', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Using the news article titles, calculate the top five people mentioned on election day\n",
        "# November 8th, 2016? Briefly comment on your findings.\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "c = Counter(['a', 'a', 'b', 'b', 'b'])\n",
        "print(c)\n",
        "def count_nouns(doc):\n",
        "  nouns = [\n",
        "      token.lemma_ for token in doc if \n",
        "            (not token.is_stop and\n",
        "             not token.is_punct and\n",
        "             token.tag_ == \"PERSON\")]\n",
        "  word_freq = Counter(nouns)\n",
        "  return word_freq\n",
        "\n",
        "counts = real_docs.apply(count_nouns).sum().most_common(20)\n",
        "print(counts)\n",
        "# # print(fake_docs.apply(count_nouns).sum().most_common(20))\n"
      ],
      "metadata": {
        "id": "vHlTFKQl0X4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create histograms of real and fake article sentiment mentioning the top person (from\n",
        "# the previous analysis). Briefly comment on your findings.\n",
        "# ᓚᘏᗢ\n",
        " "
      ],
      "metadata": {
        "id": "WKOIRYD60Zm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Plot histograms for real and fake news article sentiment (by title) for a 2 month period\n",
        "# of your choosing. Briefly comment on your findings.\n"
      ],
      "metadata": {
        "id": "xMTQSI-a0bQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Perform 1 additional analysis of your choosing on some date range/date of your choosing.\n",
        "# Justify your analysis choice, and comment on your findings.\n"
      ],
      "metadata": {
        "id": "gbWJRtVY0ciX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Time running a Spacy language model on 5 full text news articles. Use back of the\n",
        "# napkin math to calculate how long it would take to run the language model on the full\n",
        "# text of one week of articles. Briefly comment on your findings.\n"
      ],
      "metadata": {
        "id": "gr_d2Ekd0d5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Perform analyses 2-5 on the full text of a single week’s real and fake news articles. Are\n",
        "# there any differences between the two corpora discernable from these analyses? Discuss\n",
        "# any limitations of these analyses to uncover differences between the two corpora. Suggest\n",
        "# additional analyses that might uncover meaningful differences, between the two corpora\n",
        "# and salient qualities of each corpora."
      ],
      "metadata": {
        "id": "qswUt5FV0fSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}