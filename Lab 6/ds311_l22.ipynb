{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering\n",
        "- Clustering is an unsupervised task\n",
        "- Goal is assign points to \"clusters\" -- items in the same cluster are similar in some way\n",
        "- Clusters typically associated with a identifier; e.g., $1,2,3,\\dots,K$\n",
        "  - ID itself doesn't have any significance\n",
        "- You categorize clustering algorithms as:\n",
        "  - Transductive clustering algorithms: partition a finite set (training set) into disjoint sets (aka clusters). These algorithms cannot cluster new points were not present during training. Example: agglomerative clustering.\n",
        "  - Inductive clustering algorithms, which learn from the training set a partitioning of all inputs, so they can cluster new points. Example: k-means clustering.\n",
        "- Why cluster?\n",
        "  - Most often, to better understand your data and the patterns or structure therein.\n",
        "    - E.g., a company might discover that there are discrete \"types\" of customers, so it could customize how it serves each type.\n",
        "  - Sometimes, to discretize a continuous value.\n",
        "    - E.g. RGB space into blue, red, purple, etc.\n",
        "  - Rarely, as a first step in a un- or semi-supervised learning process where (1) form clusters and then (2) use heuristics or a limited amount of labeled data to assign class labels to clusters.\n",
        "  - This list is not exhaustive...\n",
        "- Challenges?\n",
        "  - There are many logical ways a set of points could be clustered.\n",
        "    - E.g., clustering pictures of shapes. Could learn to cluster based on color, shape, size, something else. Very difficult to \"steer.\"\n",
        "- Major approaches:\n",
        "  - *Centroid-based*: each cluster is defined by some *centroid* (central point) or prototype; e.g., k-means. Training means finding the right centroids.\n",
        "  - *Agglomerative*: start with data point in its own cluster; repeatedly merge \"most similar\" clusters until some stopping criterion (e.g. number of clusters, or similarity between clusters) is met.\n",
        "  - *Divisive*: start with all points in one cluster; repeatedly split until some stopping criterion is met.\n",
        "- *Hierarchical clustering algorithms* produce a tree-shaped series of clusters, where some clusters are subclusters of other clusters. E.g., one layer might cluster by species, and there may be subclusters with different breeds or varieties of these species.\n",
        "  - Agglomerative and divise approaches are hierarchical if you keep track of the intermediate clusters as you are merging or splitting, respectively."
      ],
      "metadata": {
        "id": "JZN4WkKxQRjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance Metrics Between Points\n",
        "\n",
        "Before we look at either k-means or agglomerative, we ned to talk about the concept of \"distance\" between points and the \"distance\" between clusters.\n",
        "\n",
        "Three common metrics:\n",
        "1. Squared Euclidean distance: $d(x,x') = \\sum_i (x_i - x'_i)^2$.\n",
        "2. L1 aka city block distance: $d(x,x')= \\sum_i |x_i-x'_i|$.\n",
        "3. Hamming distance (for categorical variables): $d(x,x') = \\sum_i I(x_i \\neq x_i')$\n",
        "  - $I(x_i \\neq x_i')$ is 1 if $x_i$ and $x_i'$ are different; else 0\n",
        "- ***ABCD***: If $x$ and $x'$ are binary vectors, which of the above distance metrics would give the same answer?\n",
        "  - A: None do (all give different answers)\n",
        "  - B: 1 and 2\n",
        "  - C: 2 and 3\n",
        "  - D: 1 and 2 and 3\n",
        "  - White: none of the above\n",
        "- In some cases, instead of minimizing distances you may want to maximize similarity between two points.\n",
        "- The *cosine similarity* between $x$ and $x'$:\n",
        "  $$ sim(x,x') = \\cos \\theta = \\frac{x^T x'}{\\sqrt{(x^T x) (x'^T x')}} $$\n",
        "  - Max is 1 when the two vectors are exactly aligned\n",
        "  - Min is -1 when they point in opposite directions"
      ],
      "metadata": {
        "id": "azA-DzQ5YLAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-Means Clustering Algorithm\n",
        "- A very simple iterative, \"flat\" clustering algorithm\n",
        "- $K$ is the number of clusters, which is a hyperparameter that you must choose\n",
        "- Algorithm:\n",
        "  0. Initialize $K$ centroids, $c_{(1)}, c_{(2)}, \\dots, c_{(K)}$ (e.g., randomly pick $K$ datapoints from the set to be clustered)\n",
        "  1. Assign each point to the nearest centroid by squared Euclidean distance:\n",
        "    $$ h(x) = \\arg\\min_i d(x,c_{(i)})$$\n",
        "  2. Update the centroids to be the mean of the points in the cluster\n",
        "  $$ c_{(i)} = \\frac{1}{N_i} \\sum_{x : h(x)=i} x $$\n",
        "    - Here $N_i$ is the number of points in cluster $i$\n",
        "    - The subscript stuff means we want just the $x$s that are in cluster $i$\n",
        "  3. If any points swapped clusters in Step 1a go to Step 1\n",
        "\n",
        "- ***Demo: https://www.naftaliharris.com/blog/visualizing-k-means-clustering/***"
      ],
      "metadata": {
        "id": "X-wELy3Xb9RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-Means (continued)\n",
        "- How to pick $K$? How to assess the quality of a clustering?\n",
        "  - May have some domain knowledge that suggests a particular number of sub-populations.\n",
        "  - If sufficiently low dimension (2 or 3d), visual inspection\n",
        "  - Plot a curve, where the y-axis is \"average distance between points and their centroid\" (aka \"total distortion\") and the x-axis is $K$. Pick an \"elbow\" in that curve. YMMV.\n",
        "  - Inspect cluster statistics.\n",
        "  - Downstream task performance, if there is one. (E.g. after discretizing colors).\n",
        "- $h(x)$ can cluster new points; partitions the input space into polytopes"
      ],
      "metadata": {
        "id": "BC8IzgQx0lCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Agglomerative Clustering\n",
        "- Algorithm\n",
        "  0. Initialize by placing each point it its own cluster\n",
        "  1. Find the two clusters with the minimum distance and merge them into one\n",
        "  2. While stopping criterion is not met, go to 1\n",
        "\n",
        "***In-class exercise: run agglomerative clustering with squared Euclidean distance and complete link on the provided example.***"
      ],
      "metadata": {
        "id": "7goO8ElB0nO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering with sklearn\n",
        "\n",
        "[Docs](https://scikit-learn.org/stable/modules/clustering.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "5XqXrznT1DiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Silhouette coefficient (score)\n",
        "\n",
        "Assumes that on average, points in same cluster should be closer to each other than points in different clusters. \n",
        "\n",
        "+ $a$: The mean distance between a sample and all other points in the same cluster.\n",
        "\n",
        "+ $b$: The mean distance between a sample and all other points in the next nearest cluster.\n",
        "\n",
        "The Silhouette Coefficients for a single sample is then given as:\n",
        "$$ \\frac{b - a}{\\texttt{max}(a, b)}$$\n",
        " \n",
        "The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.\n"
      ],
      "metadata": {
        "id": "KosSFhdM2VKg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUNh97CC2gwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}